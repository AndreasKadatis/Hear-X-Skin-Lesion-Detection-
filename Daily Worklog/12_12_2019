After sending my summary of work to Jaco, he advised me as to what hearX was looking at wanting to do and what their main ob-
stacle was, the skewed results as a result of the skewed datset provided from the HAM database. My first instinct was to look
at the dataset, as the model was remarkably accurate for the large dataset containg "nv" samples. My python is still patchy in 
places, especially with regards to the multiple strange libraries the code makes use of, so after working through the key func-
tions from the Sklearn library and the subsequently working through the Preprocessing module, what i had not understood fully,
but suspected while inititially trying to get the preoprocessing model to work, was the the augmentation script written by the 
original author somehow enlargens the datset to around 5000 training images for each category. My run yesterday of the augmented 
model was not able to complete so have now reduced the training size from 2000 images to 500. THe idea is not to get an exception-
ally high accuracy but one that is more spread out. It is currently running and should be done within the hour. [2.5 hours]

Worked through some tutorials on image augmentation, as this is what seems to be the most improtant part when faced with limited 
data sets, specifically played around with the keras ImageDataGenerator function and consequently worked through the augmentation 
part of the pre-processing module, attempting to understand fully what it had done. The model finally finished training and showed
improved results as to what i was expecting, with at least 30-40% accuracy all categories. I will compile these results and look
further into what preo-processing methods some of the other kernels attempted. [1.5 Hours]

Worked throug hthe Fast.ai in more detail but decided to leave it for a period as they make use of their extensive library, rather
than the more traditional tensorflow/keras methods. I then moved on to looking at the kaggle kernels, trying to gauge some diff-
erent methods with regards to augmentation. This one for example {https://www.kaggle.com/sid321axn/step-wise-approach-cnn-model-
77-0344-accuracy} reccomends the same augmentation method as the previous model me and Jaco had been discussing. This one also 
showed how to list the different meta-data categories visually, something we might want to look into. For the rest of today ill
be looking at the next ste which is the application of different models through the use of transfer learning. [1.5 hours]

Examined all the pretrained models already installed within keras, studied teh documentation on them and why transfer learning
is apprpriate in this case. Also worked through the documentation on Ensemble Models as part of a strategy workthrough for the 
the next weeks of the project, as detailed in a email summary to Jaco. [2 hours]
